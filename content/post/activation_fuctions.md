---
title: "Chapter3 활성화 함수"
date: 2020-03-16T19:34:03+09:00
draft: FALSE
tags: ["R로 딥러닝하기", "신경망", "활성화 함수"]
categories: ["R"]
---

## 활성화 함수란 ?

활성화 함수는 이전층(layer)의 뉴런에서 다음 층의 뉴런으로 신호를 전달하는 역할을 합니다. 이때 활성화 함수의 값에 따라서 다음 층의 뉴런으로 출력 신호를 보낼지 말지를 결정하게됩니다. 이제 활성화 함수의 종류에 대해 알아보겠습니다.

### 계단함수

첫 번째 활성화 함수로는 계단함수가 있습니다. 계단함수는 활설화 값을 0보다 크면 1 작으면 0을 출력하는 함수입니다. 0과 1만 출력하기 때문에 함수의 모양은 계단모양으로 보여집니다. 이 함수가 신경망에서 어떻게 작동하는지 알아보겠습니다.

먼저 2층 신경망을 떠올려 보겠습니다. 2층 신경망은 입력층과 1개의 은닉층 출력층으로 이루어진 구조입니다. 2층 신경망에서 활성화 함수는 입력-은닉층에서 신호전달 때 활성화(z1=x1W1+b1)값과 은닉-출력층에서 신호전달 때 활성화 값이 0보다 큰지 작은지를 조건으로 두번 작동하게 됩니다.

계단함수의 핵심은 활성화 값과 0과 크기 비교입니다. R에서 크기 비교는 조건문으로 가능합니다. 활성화 값이 0보다 큰 값이면 1을, 작은 값이면 0을 출력합니다. 이 조건문은 다음과 같이 `ifelse(x > 0, 1, 0)` 표현할 수 있습니다. 이 함수는 입력으로 단일한 값이 아닌 벡터를 받아도 함수가 벡터의 원소들에 대해 조건에 따라 값을 구분하여 결과를 반환해 줍니다. 코드로 정리하면 다음과 같습니다.

```{r}
step_fun <- function(x){
  return(ifelse(x > 0, 1, 0))
}
step_fun(c(-1,1,2))
```

작성한 함수를 그려보면 다음과 같이 계단 모양이 나옵니다.

```{r echo=TRUE}
#install.packages("ggplot2")
library(ggplot2)

x <- seq(from =  -5, to =  4.9, by =  0.1)
y <- step_fun(x)
data<-data.frame(x, y)
str(data)
ggplot(data, aes(x, y))+geom_line()

```

### 시그모이드 함수

계단 함수 이외에 다른 활성화 함수로는 시그모이드 함수가 있습니다. 시그모이드 함수는 0과 1 두 가지 값만 출력하는 것이 아닌 0과 1 사이의 값을 출력하는 함수입니다. 따라서 계단함수와는 달리 조건문이 따로 필요하지 않습니다. 대신 시그모이드 함수는 지수함수를 사용합니다. 시그모이드 함수의 공식은 지수함수에 음의 값을 입력받고 그 값에 1을 더한 것의 역수를 취하면 시그모이드 함수의 구현이 끝납니다.

```{r}
sigmoid <- function(x){
  return(1 / (1 + exp(-x)))
}
x <- matrix(c(-1, 1, 2),1,3)
x
sigmoid(x)
```

시그모이드 함수 그려보면 0에서 1 사이의 연속된 실수 값들이 s자 모양을 이루는 것을 확인할 수 있습니다.

```{r echo=TRUE}
x <- seq(from =  -5, to =  4.9, by =  0.1)
y <- sigmoid(x)
data<-data.frame(x, y)
str(data)
ggplot(data, aes(x, y))+geom_line()
```

시그모이드 함수와 계단함수의 차이는 시그모이드 함수는 연속적인 값이 신호로서 흐르는 반면 계단함수는 오직 2가지 값만 흐른다는 점입니다. 시그뫼드 함수의 연속적인 값들이 함수의 그래프를 s자 모양으로 만들어주며 입력 값에 비례해 출력 값이 커지는 경향성을 찾아 볼 수 있습니다. 물론 공통점을 찾아 볼 수 있습니다. 두 함수 모두 활성화 함수로서 최소 0과  최대 1 이라는 값을 출력합니다. 이 두 함수 모두 비선형 함수라는 점에서 공통점을 찾아 볼 수 있습니다. 비선형 함수란 선형 모양으로 표현할 수 없는 함수로 분수형태의 함수나 2차식 이상의 함수들을 말합니다.

활성화 함수를 비선형 함수를 사용하는 이유는, 선형 함수를 활성화 함수로 사용할 경우 층을 나누는 의미가 없어지기 때문입니다. 즉 은닉층의 의미가 없어지게 되면서 입력층과 출력층만 있는 네트워크로 표현할 수 있습니다. 다시 말해 기존의 회귀 모델등과 다를바가 없는 모델이 만들어집니다. 수학식으로 예를 들면 선형 함수 f(x)=ax와 같은 선형 함수를 중첩하면 f(f(x))의 형태가 되고 수식은 a^2x로 다시 선형 함수의 모습이 됩니다. 이에 따라 복잡한 패턴에 대해 학습하지 못하게 되고 기존의 머신러닝 분류 모델과 같은 한계를 가지게 됩니다.

### ReLU 함수

활성화 함수로 사용할 수 있는 함수에는 계단함수와 시그모이드 함수 이외에 ReLU 함수가 있습니다. ReLU 함수는 활성화 값이 0 이하면 0을 출력하고 0보다 크면 그 값을 그대로 출력하는 함수로, 비선형 함수입니다. ReLU 함수의 그래픝는 0보다 큰 범위에서는 선형모양이며, 0보다 작거나 같은 범위에서는 0으로 일자형태의 모양을 보입니다. 그렇기 때문에 ReLU 함수의 모양은 비선형 함수라 말할 수 있습니다.

```{r}
relu <- function(x){
  return(ifelse(x > 0, x, 0))
}
x <- matrix(c(-1, 1, 2),1,3)
x
relu(x)
```

이제 다른 함수들과 마찬가지로 ReLu 함수를 R로 구현하고, 그려보겠습니다.

```{r echo=TRUE}
x <- seq(from =  -5, to =  4.9, by = 0.1)
y <- relu(x)
data <- data.frame(x, y)
str(data)
ggplot(data, aes(x, y))+geom_line()
```
